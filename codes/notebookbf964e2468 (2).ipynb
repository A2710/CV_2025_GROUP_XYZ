{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a8f38efb-6f98-4898-8330-4e8e9a54dcc3",
    "_uuid": "66adae2b-245f-4adc-b911-6ce6e1763d29",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# CLIP from Scratch\n",
    "\n",
    "\n",
    "**CLIP** or **Contrastive Language-Image Pre-training** is a model that learns the relationship between a whole sentence and the image it describes; in a sense that when the model is trained, given an input sentence it will be able to retrieve the most related images corresponding to that sentence. The important thing here is that it is trained on full sentences instead of single classes like car, dog, etc. The intuition is that when trained on whole sentences, the model can learn a lot more things and finds some pattern between images and texts.\n",
    "They also show that when this model is trained on a huge dataset of images and their corresponding texts, it can also act as a classifier too. I encourage you to study the paper to learn more about this exciting model and their astonishing results on benchmarking datasets¬†. To mention just one, CLIP model trained with this strategy classifies ImageNet better than those SOTA models trained on the ImageNet itself optimized for the only task of classification!\n",
    "\n",
    "As a **teaser**, let's see what the final model that we will build in this article from scratch is capable of: given a query (raw text) like \"a boy jumping with skateboard\" or \"a girl jumping from swing\", the model will retrieve the most relevant images:\n",
    "\n",
    "![](https://i.ibb.co/9gdYqNP/teaser-cropped.png)\n",
    "\n",
    "In this notebook, we will see how to implement CLIP from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "90d3b2bc-96b4-4579-aa8c-d49d8ac300f8",
    "_uuid": "83c48ac5-2f24-4b4d-aa3f-040932a35c90",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "126a154f-2144-487f-8eca-dbeae3d37d9a",
    "_kg_hide-input": true,
    "_uuid": "fdc12344-c3ce-44d6-9309-22f3b12caaa6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:10.036347Z",
     "iopub.status.busy": "2025-03-13T10:17:10.035722Z",
     "iopub.status.idle": "2025-03-13T10:17:18.376961Z",
     "shell.execute_reply": "2025-03-13T10:17:18.375690Z",
     "shell.execute_reply.started": "2025-03-13T10:17:10.036308Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install -q timm\n",
    "# # !pip install ipywidgets==7.6.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:18.379419Z",
     "iopub.status.busy": "2025-03-13T10:17:18.379123Z",
     "iopub.status.idle": "2025-03-13T10:17:18.383488Z",
     "shell.execute_reply": "2025-03-13T10:17:18.382779Z",
     "shell.execute_reply.started": "2025-03-13T10:17:18.379390Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bbd28ae5-2880-4140-9512-ee3c1e627d7b",
    "_uuid": "823e20e3-60ab-4039-be4f-41c0fea7681a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:18.385123Z",
     "iopub.status.busy": "2025-03-13T10:17:18.384680Z",
     "iopub.status.idle": "2025-03-13T10:17:18.398246Z",
     "shell.execute_reply": "2025-03-13T10:17:18.397587Z",
     "shell.execute_reply.started": "2025-03-13T10:17:18.385085Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.862689,
     "end_time": "2021-04-05T08:01:49.835804",
     "exception": false,
     "start_time": "2021-04-05T08:01:45.973115",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:18.399966Z",
     "iopub.status.busy": "2025-03-13T10:17:18.399349Z",
     "iopub.status.idle": "2025-03-13T10:17:18.408310Z",
     "shell.execute_reply": "2025-03-13T10:17:18.407532Z",
     "shell.execute_reply.started": "2025-03-13T10:17:18.399938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# base_dir= r\"F:\\cv class project\\flickr30k_images\"\n",
    "# dataset = r\"F:\\cv class project\\flickr30k_images\\results.csv\"\n",
    "# IMG_PATH = r\"F:\\cv class project\\flickr30k_images\\flickr30k_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ba9da06-2271-4681-9f93-d778c5f84c3b",
    "_uuid": "13f2c521-f0ae-4883-a874-f6a5be15c100",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Some pre-preocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ebec2670-7a22-4ddc-bb8e-2fd234a2e8d8",
    "_uuid": "d6dfe0b8-d253-4209-b301-f98362d60b50",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:18.411412Z",
     "iopub.status.busy": "2025-03-13T10:17:18.411154Z",
     "iopub.status.idle": "2025-03-13T10:17:19.211214Z",
     "shell.execute_reply": "2025-03-13T10:17:19.210226Z",
     "shell.execute_reply.started": "2025-03-13T10:17:18.411388Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.449907,
     "end_time": "2021-04-05T08:01:51.298515",
     "exception": false,
     "start_time": "2021-04-05T08:01:49.848608",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Path to your JSON file\n",
    "json_path = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\ICFG-PEDES.json\"\n",
    "\n",
    "# Base directory where images are actually stored\n",
    "base_img_dir = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\imgs\\test\"\n",
    "\n",
    "# Load the JSON dataset\n",
    "with open(json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def clean_path(path):\n",
    "    # Remove all occurrences of 'train/'\n",
    "    path = path.replace(\"train/\", \"\")\n",
    "    # Replace multiple 'test/' occurrences with a single one\n",
    "    path = re.sub(r'(test/)+', 'test/', path)\n",
    "    # Remove the initial 'test/' since base_img_dir already includes it\n",
    "    if path.startswith(\"test/\"):\n",
    "        path = path[len(\"test/\"):]\n",
    "    # Join with base directory and normalize\n",
    "    return os.path.normpath(os.path.join(base_img_dir, path))\n",
    "\n",
    "# Build the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"image\": [clean_path(item[\"file_path\"]) for item in data],\n",
    "    \"caption\": [item[\"captions\"][0] for item in data]\n",
    "})\n",
    "\n",
    "# Add unique IDs\n",
    "df[\"id\"] = range(len(df))\n",
    "\n",
    "# Save to CSV\n",
    "output_path = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\captions.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Preprocessing complete! File saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\captions.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Check if the file exists for each image path\n",
    "df[\"valid_path\"] = df[\"image\"].apply(lambda x: os.path.exists(x))\n",
    "\n",
    "# Count valid and invalid paths\n",
    "valid_count = df[\"valid_path\"].sum()\n",
    "invalid_count = len(df) - valid_count\n",
    "\n",
    "print(f\"‚úÖ Valid image paths: {valid_count}\")\n",
    "print(f\"‚ùå Invalid image paths: {invalid_count}\")\n",
    "\n",
    "# Optionally: Save invalid paths to a file for inspection\n",
    "invalid_df = df[~df[\"valid_path\"]]\n",
    "invalid_output_path = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\invalid_paths.csv\"\n",
    "invalid_df.to_csv(invalid_output_path, index=False)\n",
    "\n",
    "print(f\"üìÅ Invalid paths saved to: {invalid_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\captions.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Check if the file exists for each image path\n",
    "df[\"valid_path\"] = df[\"image\"].apply(lambda x: os.path.exists(x))\n",
    "\n",
    "# Filter only valid rows\n",
    "valid_df = df[df[\"valid_path\"]].drop(columns=[\"valid_path\"])  # drop helper column\n",
    "\n",
    "# Save only the valid paths\n",
    "cleaned_output_path = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\captions_cleaned.csv\"\n",
    "valid_df.to_csv(cleaned_output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Cleaned data saved with {len(valid_df)} valid rows at: {cleaned_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ec804d57-53df-4742-b3b3-6477f5fb1ce7",
    "_uuid": "f7c12a2a-b8e6-4b45-81f3-dd3e76817501",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:19.213175Z",
     "iopub.status.busy": "2025-03-13T10:17:19.212721Z",
     "iopub.status.idle": "2025-03-13T10:17:19.223637Z",
     "shell.execute_reply": "2025-03-13T10:17:19.222801Z",
     "shell.execute_reply.started": "2025-03-13T10:17:19.213122Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4b5bcbda-7acd-4325-814f-06a995302794",
    "_uuid": "f2bd4d97-62be-4eda-ac01-ee2ce17b729d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012732,
     "end_time": "2021-04-05T08:01:51.324144",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.311412",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "be98f652-70cc-4d30-9cf4-12056a6c87c5",
    "_uuid": "379a777e-cb21-4f8d-ac7c-726734fe79a3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:19.224968Z",
     "iopub.status.busy": "2025-03-13T10:17:19.224680Z",
     "iopub.status.idle": "2025-03-13T10:17:19.233573Z",
     "shell.execute_reply": "2025-03-13T10:17:19.232716Z",
     "shell.execute_reply.started": "2025-03-13T10:17:19.224923Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.377383,
     "end_time": "2021-04-05T08:01:51.714313",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.33693",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class CFG:\n",
    "#     debug = False\n",
    "#     image_path = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\imgs\\test\"\n",
    "#     captions_path = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\captions_cleaned.csv\"\n",
    "#     batch_size = 64\n",
    "#     num_workers = 0\n",
    "#     head_lr = 1e-3\n",
    "#     image_encoder_lr = 1e-4\n",
    "#     text_encoder_lr = 1e-5\n",
    "#     weight_decay = 1e-3\n",
    "#     patience = 1\n",
    "#     factor = 0.8\n",
    "#     epochs = 1\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#     model_name = 'resnet50'\n",
    "#     image_embedding = 2048\n",
    "#     text_encoder_model = \"distilbert-base-uncased\"\n",
    "#     text_embedding = 768\n",
    "#     text_tokenizer = \"distilbert-base-uncased\"\n",
    "#     max_length = 200\n",
    "\n",
    "#     pretrained = True # for both image encoder and text encoder\n",
    "#     trainable = True # for both image encoder and text encoder\n",
    "#     temperature = 1.0\n",
    "\n",
    "#     # image size\n",
    "#     size = 224\n",
    "\n",
    "#     # for projection head; used for both image and text encoders\n",
    "#     num_projection_layers = 1\n",
    "#     projection_dim = 256 \n",
    "#     dropout = 0.1\n",
    "# cfg = CFG()\n",
    "\n",
    "class CFG:\n",
    "    debug = False\n",
    "    image_path = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\imgs\\test\"\n",
    "    captions_path = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\captions_cleaned.csv\"\n",
    "    batch_size = 64\n",
    "    num_workers = 0\n",
    "    epochs = 1\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Using pretrained CLIP model\n",
    "    pretrained = True  # For both image encoder and text encoder\n",
    "    trainable = True   # For both image encoder and text encoder\n",
    "\n",
    "    temperature = 1.0  # You can adjust this for contrastive loss\n",
    "    size = 224         # Default image size for CLIP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ddc0d9a6-651c-4964-b664-e37875918f3a",
    "_uuid": "7d58046c-da45-4b0c-a14e-9326e9e489b1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012746,
     "end_time": "2021-04-05T08:01:51.74007",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.727324",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bfde7601-05c8-44c6-8cbf-755a84bfa6f8",
    "_uuid": "609a1e71-a1b0-47df-ae00-0e4be4c440e5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:19.235013Z",
     "iopub.status.busy": "2025-03-13T10:17:19.234711Z",
     "iopub.status.idle": "2025-03-13T10:17:19.244673Z",
     "shell.execute_reply": "2025-03-13T10:17:19.243855Z",
     "shell.execute_reply.started": "2025-03-13T10:17:19.234972Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023459,
     "end_time": "2021-04-05T08:01:51.776328",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.752869",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7d223acd-aea0-4dfd-ab8b-f8078d78d9d4",
    "_uuid": "b81c0135-3c2e-451b-900e-077f77ead1d5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012817,
     "end_time": "2021-04-05T08:01:51.802043",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.789226",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "09f9e53b-5546-4814-b5e8-8949a1283096",
    "_uuid": "3a2d27cd-3319-4e05-8260-4ab4bb15a67b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "We need to encode both images and their describing texts. We use **Flickr 30k** dataset that contains 31.8k images and caption pairs. \n",
    "\n",
    "We will use **DistilBERT** model (which is smaller than BERT but performs nearly as well as BERT) from **HuggingFace** library as our text encoder; so, we need to **tokenize** the sentences (captions) with DistilBERT tokenizer and then feed the token ids (input_ids) and the attention masks to DistilBERT. Therefore, the dataset needs to take care of the tokenization as well. Below you can see the dataset's code. Below that I'll explain the most important things that is happening in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ded19451-b8ce-4be8-9951-b1bc42da0ff2",
    "_uuid": "bf832096-a8e2-4167-8664-949015fde0fe",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "In the **\\_\\_init\\_\\_** we receive a tokenizer object which is actually a HuggingFace tokinzer; this tokenizer will be loaded when running the model. We are padding and truncating the captions to a specified max_length. In the **\\_\\_getitem\\_\\_** we will first load an encoded caption which is a dictionary with keys input_ids and attention_mask, make tensors out of its values and after that we will load the corresponding image, transform and augment it (if there is any!) and then we make it a tensor and put it in the dictionary with \"image\" as the key. Finally we put the raw text of the caption with the key \"caption\" in the dictionary only for visualization purposes.¬†\n",
    "\n",
    "I did not use additional data augmentations but you can add them if you want to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "17917790-5c58-404e-8f20-ee3e6803115a",
    "_uuid": "9cba255a-1a17-4b06-ad9a-a30d000d8e7e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:19.246586Z",
     "iopub.status.busy": "2025-03-13T10:17:19.245754Z",
     "iopub.status.idle": "2025-03-13T10:17:19.259739Z",
     "shell.execute_reply": "2025-03-13T10:17:19.259023Z",
     "shell.execute_reply.started": "2025-03-13T10:17:19.246559Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025532,
     "end_time": "2021-04-05T08:01:51.840523",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.814991",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def get_transforms(mode=\"train\"):\n",
    "#     if mode == \"train\":\n",
    "#         return A.Compose(\n",
    "#             [\n",
    "#                 A.Resize(cfg.size, cfg.size, always_apply=True),\n",
    "#                 A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "#             ]\n",
    "#         )\n",
    "#     else:\n",
    "#         return A.Compose(\n",
    "#             [\n",
    "#                 A.Resize(cfg.size, cfg.size, always_apply=True),\n",
    "#                 A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "\n",
    "# class CLIPDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
    "#         \"\"\"\n",
    "#         image_filenames and cpations must have the same length; so, if there are\n",
    "#         multiple captions for each image, the image_filenames must have repetitive\n",
    "#         file names\n",
    "#         \"\"\"\n",
    "\n",
    "#         self.image_filenames = image_filenames\n",
    "#         self.captions = list(captions)\n",
    "#         self.encoded_captions = tokenizer(\n",
    "#             list(captions), padding=True, truncation=True, max_length=cfg.max_length\n",
    "#         )\n",
    "#         self.transforms = transforms\n",
    "#         # Print image paths to a text file before loading\n",
    "#         with open(\"image_paths.txt\", \"w\") as f:\n",
    "#             for filename in self.image_filenames:\n",
    "#                 image_path = os.path.join(cfg.image_path, filename)\n",
    "#                 f.write(image_path + \"\\n\")\n",
    "#         print(\"Image paths saved to image_paths.txt\")\n",
    "            \n",
    "#     def __getitem__(self, idx):\n",
    "#         try:\n",
    "#             item = {\n",
    "#                 key: torch.tensor(values[idx])\n",
    "#                 for key, values in self.encoded_captions.items()\n",
    "#             }\n",
    "\n",
    "#             # Construct the full image path\n",
    "#             image_path = os.path.join(cfg.image_path, self.image_filenames[idx])\n",
    "#             print(f\"Original image path: {image_path}\")\n",
    "\n",
    "#             # Normalize the path to handle different separators (e.g., / or \\)\n",
    "#             image_path = os.path.normpath(image_path)\n",
    "\n",
    "#             # Split the path into parts for manipulation\n",
    "#             parts = image_path.split(os.sep)\n",
    "\n",
    "#             # Remove duplicate consecutive 'test' if present\n",
    "#             cleaned_parts = []\n",
    "#             for i, part in enumerate(parts):\n",
    "#                 # Avoid adding consecutive 'test' directories\n",
    "#                 if part == \"test\" and i > 0 and parts[i - 1] == \"test\":\n",
    "#                     continue\n",
    "#                 cleaned_parts.append(part)\n",
    "\n",
    "#             # Reconstruct the cleaned path\n",
    "#             image_path = os.sep.join(cleaned_parts)\n",
    "\n",
    "#             # Double-check the corrected path\n",
    "#             print(f\"Trying to load image from: {image_path}\", flush=True)\n",
    "\n",
    "#             # Read the image\n",
    "#             image = cv2.imread(image_path)\n",
    "\n",
    "#             # Handle missing/corrupt images\n",
    "#             if image is None:\n",
    "#                 print(f\"‚ö†Ô∏è Error loading image: {image_path}\", flush=True)\n",
    "#                 # Return placeholder data with an empty caption\n",
    "#                 image = np.zeros((cfg.size, cfg.size, 3), dtype=np.uint8)  # Blank image\n",
    "#                 caption = \"\"  # Empty caption\n",
    "#             else:\n",
    "#                 # Convert BGR to RGB\n",
    "#                 image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#                 caption = self.captions[idx]  # Use the original caption\n",
    "\n",
    "#             # Apply transforms if available\n",
    "#             image = self.transforms(image=image)[\"image\"]\n",
    "\n",
    "#             # Prepare the item\n",
    "#             item[\"image\"] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "#             item[\"caption\"] = caption\n",
    "\n",
    "#             return item\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing sample {idx}: {e}\")\n",
    "#             # Return placeholder data with an empty caption\n",
    "#             image = np.zeros((cfg.size, cfg.size, 3), dtype=np.uint8)  # Blank image\n",
    "#             caption = \"\"  # Empty caption\n",
    "#             return {\n",
    "#                 \"image\": torch.tensor(image).permute(2, 0, 1).float(),\n",
    "#                 \"caption\": caption,\n",
    "#             }        \n",
    "#     def __len__(self):\n",
    "#         return len(self.captions)\n",
    "\n",
    "\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(cfg.size, cfg.size, always_apply=True),\n",
    "                A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711], always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(cfg.size, cfg.size, always_apply=True),\n",
    "                A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711], always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and captions must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names\n",
    "        \"\"\"\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=cfg.max_length\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Save image paths to a text file before loading\n",
    "        with open(\"image_paths.txt\", \"w\") as f:\n",
    "            for filename in self.image_filenames:\n",
    "                image_path = os.path.join(cfg.image_path, filename)\n",
    "                f.write(image_path + \"\\n\")\n",
    "        print(\"Image paths saved to image_paths.txt\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            item = {\n",
    "                key: torch.tensor(values[idx])\n",
    "                for key, values in self.encoded_captions.items()\n",
    "            }\n",
    "\n",
    "            # Construct the full image path\n",
    "            image_path = os.path.join(cfg.image_path, self.image_filenames[idx])\n",
    "            image_path = os.path.normpath(image_path)\n",
    "\n",
    "            # Clean up path to handle duplicate 'test' directories\n",
    "            parts = image_path.split(os.sep)\n",
    "            cleaned_parts = []\n",
    "            for i, part in enumerate(parts):\n",
    "                if part == \"test\" and i > 0 and parts[i - 1] == \"test\":\n",
    "                    continue\n",
    "                cleaned_parts.append(part)\n",
    "            image_path = os.sep.join(cleaned_parts)\n",
    "\n",
    "            # Read the image\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            if image is None:\n",
    "                print(f\"‚ö†Ô∏è Error loading image: {image_path}\")\n",
    "                image = np.zeros((cfg.size, cfg.size, 3), dtype=np.uint8)  # Placeholder image\n",
    "                caption = \"\"  # Empty caption\n",
    "            else:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                caption = self.captions[idx]  # Use original caption\n",
    "\n",
    "            # Apply transformations (resize + normalization)\n",
    "            image = self.transforms(image=image)[\"image\"]\n",
    "\n",
    "            item[\"image\"] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "            item[\"caption\"] = caption\n",
    "\n",
    "            return item\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            image = np.zeros((cfg.size, cfg.size, 3), dtype=np.uint8)  # Placeholder image\n",
    "            caption = \"\"  # Empty caption\n",
    "            return {\n",
    "                \"image\": torch.tensor(image).permute(2, 0, 1).float(),\n",
    "                \"caption\": caption,\n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "755ad036-a708-4007-a65f-b23ea615a9c3",
    "_uuid": "ff97dfbb-f9f9-4c27-aa11-10adaa1eb537",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012853,
     "end_time": "2021-04-05T08:01:51.866433",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.85358",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "source": [
    "## Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b06bdfc7-df5d-4da5-8d44-48ed21ac827a",
    "_uuid": "60721627-97b1-4d8d-b7a4-da326c594bdc",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "The image encoder code is straight forward. I'm using PyTorch Image Models library (timm) here which makes a lot of different image models available from ResNets to EfficientNets and many more. Here we will use a ResNet50 as our image encoder. You can easily use torchvision library to use ResNets if you don't want to install a new library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "172afffd-db12-48f5-8523-29865b2dc03b",
    "_uuid": "779c353d-3bce-4555-93f4-4a96b5813445",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "The code encodes each image to a fixed size vector with the size of the model's output channels (in case of ResNet50 the vector size will be **2048**). This is the output after the nn.AdaptiveAvgPool2d() layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e919730d-93e5-4e98-ad60-0359ecbe5320",
    "_uuid": "ae9cae40-ade2-4108-8bc9-f8c093091933",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:19.261463Z",
     "iopub.status.busy": "2025-03-13T10:17:19.260878Z",
     "iopub.status.idle": "2025-03-13T10:17:19.274947Z",
     "shell.execute_reply": "2025-03-13T10:17:19.274223Z",
     "shell.execute_reply.started": "2025-03-13T10:17:19.261425Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.027706,
     "end_time": "2021-04-05T08:01:51.907283",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.879577",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class ImageEncoder(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Encode images to a fixed size vector\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self, model_name=cfg.model_name, pretrained=cfg.pretrained, trainable=cfg.trainable\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.model = timm.create_model(\n",
    "#             model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "#         )\n",
    "#         for p in self.model.parameters():\n",
    "#             p.requires_grad = trainable\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images using CLIP's image encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=cfg.pretrained, trainable=cfg.trainable):\n",
    "        super().__init__()\n",
    "        # Load the pretrained CLIP model\n",
    "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device=cfg.device)  # You can choose other CLIP architectures here\n",
    "        \n",
    "        # Set the image encoder part of CLIP to be trainable or frozen\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The image preprocessing is applied here for CLIP\n",
    "        x = self.clip_preprocess(x).to(cfg.device)\n",
    "        return self.clip_model.encode_image(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9243350c-4e36-466c-a85d-8a430fb18ead",
    "_uuid": "18362385-7699-47c6-8bd9-8362c87ff401",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Text Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c4395ba4-7982-4e5d-97f1-89d23b4a843f",
    "_uuid": "d7eb7d5e-ef2b-4d42-bb99-388d94a65ed7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "I'll use DistilBERT as the text encoder. Like its bigger brother BERT, two special tokens will be added to the actual input tokens: **CLS** and **SEP** which mark the start and end of a sentence. To grab the whole representation of a sentence (as the related BERT and DistilBERT papers point out) we use the final representations of the CLS token and we hope that this representation captures the overall meaning of the sentence (caption). Thinking it in this way, it is similar to what we did to images and converted them into a fixed size vector.\n",
    "\n",
    "In the case of DistilBERT (and also BERT) the output hidden representation for each token is a vector with size **768**. So, the whole caption will be encoded in the CLS token representation whose size is 768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "11a102d9-c0ff-4d97-b1cc-da55b594bfb8",
    "_uuid": "dc6c39e5-d3e2-4c3e-95a5-74934a7fbb0b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:19.276110Z",
     "iopub.status.busy": "2025-03-13T10:17:19.275817Z",
     "iopub.status.idle": "2025-03-13T10:17:19.287716Z",
     "shell.execute_reply": "2025-03-13T10:17:19.286950Z",
     "shell.execute_reply.started": "2025-03-13T10:17:19.276083Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.027706,
     "end_time": "2021-04-05T08:01:51.907283",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.879577",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class TextEncoder(nn.Module):\n",
    "#     def __init__(self, model_name=cfg.text_encoder_model, pretrained=cfg.pretrained, trainable=cfg.trainable):\n",
    "#         super().__init__()\n",
    "#         if pretrained:\n",
    "#             self.model = DistilBertModel.from_pretrained(model_name)\n",
    "#         else:\n",
    "#             self.model = DistilBertModel(config=DistilBertConfig())\n",
    "            \n",
    "#         for p in self.model.parameters():\n",
    "#             p.requires_grad = trainable\n",
    "\n",
    "#         # we are using the CLS token hidden representation as the sentence's embedding\n",
    "#         self.target_token_idx = 0\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         last_hidden_state = output.last_hidden_state\n",
    "#         return last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b76675f-63cc-4361-afb4-e61f0352e65e",
    "_uuid": "345b6206-3c8b-4a38-b587-0bac99c9a3e0",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Projection Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "545452f1-b70f-4852-8b6d-85a448f58601",
    "_uuid": "94ba92fc-812d-487e-92dd-26115369b234",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "Now that we have encoded both our images and texts into fixed size vectors (2048 for image and 768 for text) we need to bring (project) them into a _new world_ with **similar dimensions** for both images and texts in order to be able to compare them and push apart the non-relevant image and texts and pull together those that match. So, the following code will bring the 2048 and 768 dimensional vectors into a 256 (projection_dim) dimensional world, where we can **compare** them.\n",
    "\n",
    "\"embedding_dim\" is the size of the input vector (2048 for images and 768 for texts) and \"projection_dim\" is the the size of the output vector which will be 256 for our case. For understanding the details of this part you can refer to the CLIP paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "966bc2b1-d27d-44db-889c-8b446c5f33ca",
    "_uuid": "9fc1ddd0-4585-4070-b569-3c730fedda0c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:19.289075Z",
     "iopub.status.busy": "2025-03-13T10:17:19.288729Z",
     "iopub.status.idle": "2025-03-13T10:17:19.299248Z",
     "shell.execute_reply": "2025-03-13T10:17:19.298541Z",
     "shell.execute_reply.started": "2025-03-13T10:17:19.289033Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.027706,
     "end_time": "2021-04-05T08:01:51.907283",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.879577",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=cfg.projection_dim,\n",
    "        dropout=cfg.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a6261325-8971-4478-a7e0-d31218bdffd7",
    "_uuid": "b688e799-3dba-46b3-b6e6-4fa9244af817",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012961,
     "end_time": "2021-04-05T08:01:51.933336",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.920375",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f5f736ff-f3d8-48f0-ae4e-02ba2695c1df",
    "_uuid": "354eb6e0-b013-4969-b0c7-af4c53cfe5d6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "![clip.png](attachment:fb0403a4-73b2-4b97-bfec-c824d11677ee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "115f339d-bd4c-4885-8693-619300138221",
    "_uuid": "7da598fd-070b-4dad-8b34-41e4dcb5f104",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "Here we will use the previous modules that we built to implement the main model. The \\_\\_init\\_\\_ function is self-explanatory. In the forward function, we first encode the images and texts separately into fixed size vectors (with different dimensionalities). After that, using separate projection modules we project them to that shared world (space) that I talked about previously. Here the encodings will become of similar shape (256 in our case). After that we will compute the loss. Again I recommend reading CLIP paper to get it better but I'll try my best to explain this part.\n",
    "\n",
    "In Linear Algebra, one common way to measure if two vectors are of similar characteristics (they are like each other) is to calculate their **dot product** (multiplying the matching entries and take the sum of them); if the final number is big, they are alike and if it is small they are not (relatively speaking)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "341316f7-138f-48cb-9f00-01bd4493ac69",
    "_uuid": "de1dc1f8-cd7f-4a47-9953-7ed4f769df52",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "Let's now understand the loss function. We talked about two vectors, but, what do we have here? We have image_embeddings, a matrix with shape (batch_size, 256) and text_embeddings with shape (batch_size, 256). It means we have two groups of vectors instead of two single vectors. How do we measure how similar two groups of vectors (two matrices) are to each other? Again, with dot product (@ operator in PyTorch does the dot product or matrix multiplication in this case). To be able to multiply these two matrices together, we transpose the second one. Okay, we get a matrix with shape (batch_size, batch_size) which we will call logits. (temperature is equal to 1.0 in our case, so, it does not make a difference. You can play with it and see what difference it makes. Also look at the paper to see why it is here!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4791b85b-a9ed-4693-94fd-f7b515590e06",
    "_uuid": "45c0f8ce-3c04-43d4-b51f-0af895f679e8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:19.300402Z",
     "iopub.status.busy": "2025-03-13T10:17:19.300159Z",
     "iopub.status.idle": "2025-03-13T10:17:19.310464Z",
     "shell.execute_reply": "2025-03-13T10:17:19.309799Z",
     "shell.execute_reply.started": "2025-03-13T10:17:19.300372Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025366,
     "end_time": "2021-04-05T08:01:51.972338",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.946972",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class CLIPModel(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         temperature=cfg.temperature,\n",
    "#         image_embedding=cfg.image_embedding,\n",
    "#         text_embedding=cfg.text_embedding,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.image_encoder = ImageEncoder()\n",
    "#         self.text_encoder = TextEncoder()\n",
    "#         self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "#         self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "#         self.temperature = temperature\n",
    "\n",
    "#     def forward(self, batch):\n",
    "#         # Getting Image and Text Features\n",
    "#         image_features = self.image_encoder(batch[\"image\"])\n",
    "#         text_features = self.text_encoder(\n",
    "#             input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "#         )\n",
    "#         # Getting Image and Text Embeddings (with same dimension)\n",
    "#         image_embeddings = self.image_projection(image_features)\n",
    "#         text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "#         # Calculating the Loss\n",
    "#         logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "#         images_similarity = image_embeddings @ image_embeddings.T\n",
    "#         texts_similarity = text_embeddings @ text_embeddings.T\n",
    "#         targets = F.softmax(\n",
    "#             (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "#         )\n",
    "#         texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "#         images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "#         loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "#         return loss.mean()\n",
    "\n",
    "\n",
    "# def cross_entropy(preds, targets, reduction='none'):\n",
    "#     log_softmax = nn.LogSoftmax(dim=-1)\n",
    "#     loss = (-targets * log_softmax(preds)).sum(1)\n",
    "#     if reduction == \"none\":\n",
    "#         return loss\n",
    "#     elif reduction == \"mean\":\n",
    "#         return loss.mean()\n",
    "\n",
    "from transformers import CLIPModel\n",
    "\n",
    "class CLIPWrapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.clip(**inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "90912fca-1621-4957-bbe7-622623818f44",
    "_uuid": "6f41db69-005e-4c5c-8aae-eeda1d73cf04",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "So, in the best case scenario, text_embeddings and image_embedding matricies should be the same because they are describing similar things. Let's think now: if this happens, what would the logits matrix be like? Let's see with a simple example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2e7b03d7-0bc8-4d4a-b621-6be3f261ca8c",
    "_uuid": "9e79758d-0025-4dae-91d8-8c4976e9a040",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:19.313351Z",
     "iopub.status.busy": "2025-03-13T10:17:19.313100Z",
     "iopub.status.idle": "2025-03-13T10:17:19.327511Z",
     "shell.execute_reply": "2025-03-13T10:17:19.326660Z",
     "shell.execute_reply.started": "2025-03-13T10:17:19.313326Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# A simple Example\n",
    "\n",
    "batch_size = 4\n",
    "dim = 256\n",
    "embeddings = torch.randn(batch_size, dim)\n",
    "out = embeddings @ embeddings.T\n",
    "print(F.softmax(out, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "86427abd-62db-451a-a9f8-df83cb07147b",
    "_uuid": "db5df34f-f91d-41af-866f-1f5a5b36fba6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "So logits, in the best case, will be a matrix that if we take its softmax, will have 1.0s in the diagonal (An identity matrix to call it with fancy words!). As the loss function's job is to make model's predictions similar to targets (at least in most cases!), we want such a matrix as our target. That's the reason why we are calculating images_similarity and texts_similarity matrices in the code block above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "464524dd-c7b9-42ee-87a2-b1f1f8251db8",
    "_uuid": "5908ec0e-5de4-4d5a-9515-46bd6ef6f54e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "Now that we've got our targets matrix, we will use simple cross entropy to calculate the actual loss. I've written the full matrix form of cross entropy as a function which you can see in the bottom of the code block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "90e12738-c1a2-4a2d-84ea-6448d54fb786",
    "_uuid": "4cebe696-a383-4a05-aa9d-ae8ca96a30f7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "There's a simpler way to calculate this loss in PyTorch; by doing this: nn.CrossEntropyLoss()(logits, torch.arange(batch_size)). The reason of not using that here is that the dataset we are using has multiple captions for a single image; so, there is the possibility that two identical images with their similar captions exist in a batch (it is rare but it can happen). Taking the loss with this easier method will ignore this possibility and the model learns to pull apart two representations (assume them different)  that are actually the same. Obviously, we don't want this to happen so I calculated the whole target matrix in a way that takes care of these edge cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b6256b72-e53d-42d6-bf4b-9ac307292e5d",
    "_uuid": "912d541b-d122-491a-ac8d-882d81a5b194",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013097,
     "end_time": "2021-04-05T08:01:51.998635",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.985538",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e469c887-6d40-469f-aadd-812f56c79fa9",
    "_uuid": "4a97bad3-3fc0-412a-9a8d-821315211124",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "Here are some funtions to help us load train and valid dataloaders, our model and then train and evaluate our model on those. There's not much going on here; just simple training loop and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6ac08d5f-1447-4859-8185-6dc52ddb26cd",
    "_uuid": "5156b3f3-11fb-4216-aa6b-99ce77e75f73",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:19.328770Z",
     "iopub.status.busy": "2025-03-13T10:17:19.328453Z",
     "iopub.status.idle": "2025-03-13T10:17:19.336119Z",
     "shell.execute_reply": "2025-03-13T10:17:19.335214Z",
     "shell.execute_reply.started": "2025-03-13T10:17:19.328743Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.041512,
     "end_time": "2021-04-05T08:01:52.054352",
     "exception": false,
     "start_time": "2021-04-05T08:01:52.01284",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_train_valid_dfs():\n",
    "    dataframe = pd.read_csv(r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\captions_cleaned.csv\")\n",
    "    max_id = dataframe[\"id\"].max() + 1 if not cfg.debug else 100\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe\n",
    "\n",
    "\n",
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    transforms = get_transforms(mode=mode)\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"image\"].values,\n",
    "        dataframe[\"caption\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        num_workers=cfg.num_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4fcf43cb-9489-41cb-8fa6-bd85a6902464",
    "_uuid": "c8f9abe5-f3de-4a27-9cf5-a306de2d7478",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "Here's a handy function to train our model. There's not much happening here; just loading the batches, feeding them to the model and stepping the optimizer and lr_scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ccfd828f-ef6b-4648-a2fd-3931c481e8c9",
    "_uuid": "c0e75961-8551-4234-8ec6-e38f9a68c766",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:19.337497Z",
     "iopub.status.busy": "2025-03-13T10:17:19.337244Z",
     "iopub.status.idle": "2025-03-13T10:17:19.349699Z",
     "shell.execute_reply": "2025-03-13T10:17:19.348831Z",
     "shell.execute_reply.started": "2025-03-13T10:17:19.337472Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.041512,
     "end_time": "2021-04-05T08:01:52.054352",
     "exception": false,
     "start_time": "2021-04-05T08:01:52.01284",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(cfg.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n",
    "\n",
    "\n",
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(cfg.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0b208705-3f13-4598-aaca-5501660f6414",
    "_uuid": "24a9974d-70a3-49d6-98e9-3a2eb996c2aa",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "Running the next cell start training the model. Put the kernel on GPU mode. Every epoch should take about 24 minutes on GPU (even one epoch is enough!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fe9cb28e-3143-4c84-8bc5-eca8a29f5aff",
    "_uuid": "d9608ec3-4cd0-4aa4-a52f-3c75ee2036cb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T10:17:19.350958Z",
     "iopub.status.busy": "2025-03-13T10:17:19.350657Z",
     "iopub.status.idle": "2025-03-13T10:24:15.807802Z",
     "shell.execute_reply": "2025-03-13T10:24:15.806502Z",
     "shell.execute_reply.started": "2025-03-13T10:17:19.350920Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_df, valid_df = make_train_valid_dfs()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "\n",
    "model = CLIPModel().to(CFG.device)\n",
    "\n",
    "params = [\n",
    "    {\"params\": model.image_encoder.parameters(), \"lr\": cfg.image_encoder_lr},\n",
    "    {\"params\": model.text_encoder.parameters(), \"lr\": cfg.text_encoder_lr},\n",
    "    {\"params\": itertools.chain(\n",
    "        model.image_projection.parameters(), model.text_projection.parameters()\n",
    "    ), \"lr\": cfg.head_lr, \"weight_decay\": cfg.weight_decay}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", patience=cfg.patience, factor=cfg.factor\n",
    ")\n",
    "step = \"epoch\"\n",
    "\n",
    "best_loss = float('inf')\n",
    "for epoch in range(cfg.epochs):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    model.train()\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = valid_epoch(model, valid_loader)\n",
    "\n",
    "    if valid_loss.avg < best_loss:\n",
    "        best_loss = valid_loss.avg\n",
    "        torch.save(model.state_dict(), \"best1.pt\")\n",
    "        print(\"Saved Best Model!\")\n",
    "\n",
    "    lr_scheduler.step(valid_loss.avg)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # \"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\imgs\\test\\0000\\0000_004_01_0303morning_0017_1.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bfa8749b-a11e-452b-b34c-ab21ea3445c5",
    "_uuid": "ec983284-efaa-4523-8178-16b420fe5a3c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d2295e9-e053-436f-8a19-2fe9a4db6275",
    "_uuid": "72bf71c3-8792-475a-ade0-7954f3ec390f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "Okay! We are done with training the model. Now, we need to do inference which in our case will be giving the model a piece of text and want it to retrieve the most relevant images from an unseen validation (or test) set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e8d53ff0-b8e8-4ce4-8c99-9e16d4e66724",
    "_uuid": "9b0dfe0d-2d7d-4e7a-b5b1-8d1272a880e4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Getting Image Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a4e6afd2-b16d-47a5-a6de-fc83ffd8e6c5",
    "_uuid": "455934db-87ab-4348-9c38-cfe54c02bf72",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "In this function, we are loading the model that we saved after training, feeding it images in validation set and returning the image_embeddings with shape (valid_set_size, 256) and the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5a2c179-ae13-4aeb-94e2-c337f152fea2",
    "_uuid": "8f249e9e-1972-4131-a785-c6d1fcc5e852",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-03-13T10:24:15.809084Z",
     "iopub.status.idle": "2025-03-13T10:24:15.809564Z",
     "shell.execute_reply": "2025-03-13T10:24:15.809343Z",
     "shell.execute_reply.started": "2025-03-13T10:24:15.809318Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_image_embeddings(valid_df, model_path):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)\n",
    "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "    \n",
    "    model = CLIPModel().to(cfg.device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=cfg.device))\n",
    "    model.eval()\n",
    "    \n",
    "    valid_image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            image_features = model.image_encoder(batch[\"image\"].to(cfg.device))\n",
    "            image_embeddings = model.image_projection(image_features)\n",
    "            valid_image_embeddings.append(image_embeddings)\n",
    "    return model, torch.cat(valid_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ee2484ba-00a9-45d4-8178-c590a31d6125",
    "_uuid": "8afbfd1a-2870-49ad-8467-0df2a52cb2fc",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-03-13T10:24:15.811312Z",
     "iopub.status.idle": "2025-03-13T10:24:15.811591Z",
     "shell.execute_reply": "2025-03-13T10:24:15.811469Z",
     "shell.execute_reply.started": "2025-03-13T10:24:15.811455Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "_, valid_df = make_train_valid_dfs()\n",
    "model, image_embeddings = get_image_embeddings(valid_df, \"best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c391bff3-07ee-4cdc-b3d9-47e4d5c780aa",
    "_uuid": "063697ef-c376-4e91-aa14-91ee5c44a778",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Finding Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "93d03289-488a-4d7d-b566-56c327eb35d6",
    "_uuid": "3ccb1e43-cbd5-4707-950c-30f5e89d9088",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "This function does the final task that we wished our model would be capable of: it gets the model, image_embeddings, and a text query. It will display the most relevant images from the validation set! Isn't it amazing? Let's see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "05812e18-cd6c-46b0-9be5-e78c9ad40276",
    "_uuid": "bc4f28e1-98d7-4184-9140-5c878090aec2",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-03-13T10:24:15.813265Z",
     "iopub.status.idle": "2025-03-13T10:24:15.813577Z",
     "shell.execute_reply": "2025-03-13T10:24:15.813447Z",
     "shell.execute_reply.started": "2025-03-13T10:24:15.813430Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025647,
     "end_time": "2021-04-05T12:36:01.385717",
     "exception": false,
     "start_time": "2021-04-05T12:36:01.36007",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)\n",
    "    encoded_query = tokenizer([query])\n",
    "    batch = {\n",
    "        key: torch.tensor(values).to(cfg.device)\n",
    "        for key, values in encoded_query.items()\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        text_embeddings = model.text_projection(text_features)\n",
    "    \n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
    "    \n",
    "    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
    "    matches = [image_filenames[idx] for idx in indices[::5]]\n",
    "    \n",
    "    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    for match, ax in zip(matches, axes.flatten()):\n",
    "        image = cv2.imread(f\"{cfg.image_path}/{match}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)\n",
    "    encoded_query = tokenizer([query])\n",
    "    batch = {\n",
    "        key: torch.tensor(values).to(cfg.device)\n",
    "        for key, values in encoded_query.items()\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        text_embeddings = model.text_projection(text_features)\n",
    "\n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
    "\n",
    "    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
    "    matches = [image_filenames[idx] for idx in indices[::5]]\n",
    "\n",
    "    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    shown = 0\n",
    "\n",
    "    for match, ax in zip(matches, axes.flatten()):\n",
    "        image_path = os.path.join(cfg.image_path, match)\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"[Warning] File does not exist: {image_path}\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"[Error] Could not read image at: {image_path}\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "        shown += 1\n",
    "\n",
    "    if shown == 0:\n",
    "        print(\"[Info] No images were successfully loaded.\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c78285bf-6c13-48e3-bb3b-a33ff54f3fa4",
    "_uuid": "9aa1d131-55b9-4195-a1f9-f9be79239ffe",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "This is how we use this function. The results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "76aeb854-19f1-47bb-8aa6-c32aed917534",
    "_uuid": "ed8f9dad-a25b-4782-ba05-6816cb3846b1",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-03-13T10:24:15.815139Z",
     "iopub.status.idle": "2025-03-13T10:24:15.815444Z",
     "shell.execute_reply": "2025-03-13T10:24:15.815318Z",
     "shell.execute_reply.started": "2025-03-13T10:24:15.815303Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "find_matches(model, \n",
    "             image_embeddings,\n",
    "             query=\"A middle-aged woman with black hair tied on the back is wearing a black hooded insulated jacket with a grey round patch on front over a grey shirt. She is also wearing fitted blue denim pants.\",\n",
    "             \n",
    "             image_filenames=valid_df['image'].values,\n",
    "             n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 1: Initialize the CLIP Model\n",
    "model = CLIPModel().to(device)  # Make sure this matches your training architecture\n",
    "\n",
    "# Step 2: Load the trained weights into the model\n",
    "model.load_state_dict(torch.load(\"best.pt\", map_location=device))  # Load state_dict\n",
    "\n",
    "# Step 3: Set the model to evaluation mode\n",
    "model.eval()  # Load the best saved model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "# Function to extract features\n",
    "def extract_features(dataloader, model, feature_type=\"image\"):\n",
    "    \"\"\"Extract image or text features using the trained model.\"\"\"\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Extracting {feature_type} features\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if k != \"caption\"}\n",
    "\n",
    "            if feature_type == \"image\":\n",
    "                feature = model.image_projection(model.image_encoder(batch[\"image\"]))\n",
    "            else:  # Text Features\n",
    "                feature = model.text_projection(model.text_encoder(\n",
    "                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "                ))\n",
    "\n",
    "            feature = F.normalize(feature, dim=1)  # Normalize for cosine similarity\n",
    "            features.append(feature.cpu().numpy())\n",
    "\n",
    "    return np.vstack(features)\n",
    "\n",
    "# Load validation dataset\n",
    "_, valid_dataframe = make_train_valid_dfs()  # Get validation split\n",
    "\n",
    "# Initialize tokenizer (assuming you already have a tokenizer object)\n",
    "valid_loader = build_loaders(valid_dataframe, tokenizer, mode=\"valid\")\n",
    "\n",
    "# Extract features using validation data\n",
    "image_features = extract_features(valid_loader, model, \"image\")\n",
    "text_features = extract_features(valid_loader, model, \"text\")\n",
    "\n",
    "\n",
    "# Compute cosine similarity between image and text features\n",
    "similarity_matrix = np.matmul(text_features, image_features.T)\n",
    "\n",
    "# Function to compute retrieval metrics\n",
    "def compute_retrieval_metrics(similarity_matrix, top_k=[1, 5, 10]):\n",
    "    \"\"\"Compute Recall@K and Mean Average Precision (mAP).\"\"\"\n",
    "    num_queries = similarity_matrix.shape[0]\n",
    "    recall_at_k = {k: 0 for k in top_k}\n",
    "    average_precision = []\n",
    "    image_to_indices = valid_df.groupby('image').indices  # Map image to caption indices\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        # sorted_indices = np.argsort(similarity_matrix[i])[::-1]  # Descending order\n",
    "        # gt_index = i  # Assuming ground-truth image-text pair is at index i\n",
    "        sorted_indices = np.argsort(similarity_matrix[i])[::-1]\n",
    "        gt_image = valid_df.iloc[i]['image']\n",
    "        gt_indices = image_to_indices[gt_image]  # All caption indices for this image\n",
    "\n",
    "        # Compute Recall@K\n",
    "        # for k in top_k:\n",
    "        #     if gt_index in sorted_indices[:k]:\n",
    "        #         recall_at_k[k] += 1\n",
    "\n",
    "        # # Compute AP\n",
    "        # rank = np.where(sorted_indices == gt_index)[0][0] + 1\n",
    "        # average_precision.append(1.0 / rank)\n",
    "\n",
    "        for k in top_k:\n",
    "            if any(idx in sorted_indices[:k] for idx in gt_indices):\n",
    "                recall_at_k[k] += 1\n",
    "\n",
    "        ranks = [np.where(sorted_indices == idx)[0][0] + 1 for idx in gt_indices]\n",
    "        average_precision.append(np.mean([1.0 / rank for rank in ranks]))\n",
    "\n",
    "\n",
    "\n",
    "    # Normalize metrics\n",
    "    recall_at_k = {k: recall_at_k[k] / num_queries for k in recall_at_k}\n",
    "    mean_ap = np.mean(average_precision)\n",
    "\n",
    "    return recall_at_k, mean_ap\n",
    "\n",
    "# Compute metrics\n",
    "recall_k, mean_ap = compute_retrieval_metrics(similarity_matrix)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nRetrieval Performance:\")\n",
    "for k in recall_k:\n",
    "    print(f\"Recall@{k}: {recall_k[k]:.4f}\")\n",
    "print(f\"Mean Average Precision (mAP): {mean_ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\imgs\\test\\0000\\0000_000_01_0303morning_0015_0.jpg\"\n",
    " F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\imgs\\test\\train\\0715\\0715_009_07_0302afternoon_1291_0.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:03<00:00,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings saved to clip_embeddings.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# Load CSV\n",
    "# =========================\n",
    "csv_file = r'F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\invalid_paths.csv'\n",
    "df = pd.read_csv(csv_file, header=0)  # Ensure your CSV has header: image_path, description, id\n",
    "\n",
    "# =========================\n",
    "# Transform for CLIP\n",
    "# =========================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3),\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# Load CLIP\n",
    "# =========================\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name).cuda().eval()\n",
    "\n",
    "# =========================\n",
    "# Dataset Definition\n",
    "# =========================\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = row['image_path']\n",
    "        caption = row['description']\n",
    "        label_id = int(row['id'])\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFoundError(f\"Missing image: {image_path}\")\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Prompt-enhanced caption\n",
    "        prompt = f\"A photo of a person. {caption}\"\n",
    "        text_inputs = self.processor(\n",
    "            text=prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77\n",
    "        )\n",
    "\n",
    "        return image, text_inputs, label_id\n",
    "\n",
    "# =========================\n",
    "# Collate Function for Dataloader\n",
    "# =========================\n",
    "def collate_fn(batch):\n",
    "    images, texts, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    # Convert text dict list into batched dict\n",
    "    input_ids = torch.cat([t['input_ids'] for t in texts], dim=0)\n",
    "    attention_mask = torch.cat([t['attention_mask'] for t in texts], dim=0)\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "    return images, {'input_ids': input_ids, 'attention_mask': attention_mask}, labels\n",
    "\n",
    "# =========================\n",
    "# Dataloader\n",
    "# =========================\n",
    "dataset = ImageTextDataset(df, processor, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# =========================\n",
    "# Inference\n",
    "# =========================\n",
    "image_embeds = []\n",
    "text_embeds = []\n",
    "label_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, text_inputs, labels in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "        images = images.cuda()\n",
    "        input_ids = text_inputs['input_ids'].cuda()\n",
    "        attention_mask = text_inputs['attention_mask'].cuda()\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=images)\n",
    "\n",
    "        # Normalize\n",
    "        img_embed = F.normalize(outputs.image_embeds, p=2, dim=1)\n",
    "        txt_embed = F.normalize(outputs.text_embeds, p=2, dim=1)\n",
    "\n",
    "        image_embeds.append(img_embed.cpu())\n",
    "        text_embeds.append(txt_embed.cpu())\n",
    "        label_ids.append(labels)\n",
    "\n",
    "# =========================\n",
    "# Save Embeddings\n",
    "# =========================\n",
    "image_embeds = torch.cat(image_embeds, dim=0)\n",
    "text_embeds = torch.cat(text_embeds, dim=0)\n",
    "label_ids = torch.cat(label_ids, dim=0)\n",
    "\n",
    "torch.save({\n",
    "    \"image_embeds\": image_embeds,\n",
    "    \"text_embeds\": text_embeds,\n",
    "    \"labels\": label_ids\n",
    "}, \"clip_embeddings.pt\")\n",
    "\n",
    "print(\"‚úÖ Embeddings saved to clip_embeddings.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/1725 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\cv class project\\cv_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'image'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m image_embeds, text_embeds, label_ids = [], [], []\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExtracting embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\cv class project\\cv_env\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\cv class project\\cv_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\cv class project\\cv_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\cv class project\\cv_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mImageTextDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     38\u001b[39m     row = \u001b[38;5;28mself\u001b[39m.df.iloc[idx]\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     image = \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# PIL image\u001b[39;00m\n\u001b[32m     40\u001b[39m     caption = row[\u001b[33m'\u001b[39m\u001b[33mcaption\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     41\u001b[39m     label = row[\u001b[33m'\u001b[39m\u001b[33mlabel_id\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\cv class project\\cv_env\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1124\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\cv class project\\cv_env\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1236\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\cv class project\\cv_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'image'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(similarity_matrix, labels, k):\n",
    "    correct = 0\n",
    "    sorted_indices = np.argsort(similarity_matrix, axis=1)[:, ::-1]\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] in labels[sorted_indices[i, :k]]:\n",
    "            correct += 1\n",
    "    return correct / len(labels)\n",
    "\n",
    "def compute_mAP(similarity_matrix, labels):\n",
    "    sorted_indices = np.argsort(similarity_matrix, axis=1)[:, ::-1]\n",
    "    num_classes = len(np.unique(labels))\n",
    "    aps = []\n",
    "\n",
    "    for cls in np.unique(labels):\n",
    "        y_true = (labels == cls).astype(int)\n",
    "        y_score = np.array([\n",
    "            1 if cls in labels[sorted_indices[i, :10]] else 0\n",
    "            for i in range(len(labels))\n",
    "        ])\n",
    "        aps.append(average_precision_score(y_true, y_score))\n",
    "\n",
    "    return np.mean(aps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 0.0022\n",
      "Recall@5: 0.0079\n",
      "Recall@10: 0.0137\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecall@5: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall_at_k(similarity_matrix,\u001b[38;5;250m \u001b[39mlabels,\u001b[38;5;250m \u001b[39m\u001b[32m5\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecall@10: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall_at_k(similarity_matrix,\u001b[38;5;250m \u001b[39mlabels,\u001b[38;5;250m \u001b[39m\u001b[32m10\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmAP: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcompute_mAP\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimilarity_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mcompute_mAP\u001b[39m\u001b[34m(similarity_matrix, labels)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m np.unique(labels):\n\u001b[32m     15\u001b[39m     y_true = (labels == \u001b[38;5;28mcls\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     16\u001b[39m     y_score = np.array([\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m labels[sorted_indices[i, :\u001b[32m10\u001b[39m]] \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     18\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labels))\n\u001b[32m     19\u001b[39m     ])\n\u001b[32m     20\u001b[39m     aps.append(average_precision_score(y_true, y_score))\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(aps)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load saved embeddings\n",
    "image_embeds = np.load(\"image_embeddings.npy\")\n",
    "text_embeds = np.load(\"text_embeddings.npy\")\n",
    "labels = np.load(\"labels.npy\")\n",
    "\n",
    "# Cosine similarity\n",
    "similarity_matrix = np.dot(image_embeds, text_embeds.T)\n",
    "\n",
    "# Recall and mAP\n",
    "print(f\"Recall@1: {recall_at_k(similarity_matrix, labels, 1):.4f}\")\n",
    "print(f\"Recall@5: {recall_at_k(similarity_matrix, labels, 5):.4f}\")\n",
    "print(f\"Recall@10: {recall_at_k(similarity_matrix, labels, 10):.4f}\")\n",
    "print(f\"mAP: {compute_mAP(similarity_matrix, labels):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:03<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings saved\n",
      "Recall@1 : 0.0381\n",
      "Recall@5 : 0.0942\n",
      "Recall@10: 0.1583\n",
      "mAP      : 0.0783\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# =========================\n",
    "# Load CSV (Update your path!)\n",
    "# =========================\n",
    "csv_path = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\invalid_paths.csv\"  # Make sure it has columns: image_path, description, id\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# =========================\n",
    "# CLIP Model Setup\n",
    "# =========================\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name).cuda().eval()\n",
    "\n",
    "# =========================\n",
    "# Image Preprocessing\n",
    "# =========================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# Dataset Class\n",
    "# =========================\n",
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = row['image_path']\n",
    "        caption = row['description']\n",
    "        label_id = int(row['id'])\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        prompt = f\"A photo of a person. {caption}\"\n",
    "        text_input = self.processor(\n",
    "            text=prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77\n",
    "        )\n",
    "\n",
    "        return image, text_input, label_id\n",
    "\n",
    "# =========================\n",
    "# Collate Function\n",
    "# =========================\n",
    "def collate_fn(batch):\n",
    "    images, texts, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    input_ids = torch.cat([t['input_ids'] for t in texts], dim=0)\n",
    "    attention_mask = torch.cat([t['attention_mask'] for t in texts], dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return images, {'input_ids': input_ids, 'attention_mask': attention_mask}, labels\n",
    "\n",
    "# =========================\n",
    "# Dataloader\n",
    "# =========================\n",
    "dataset = CLIPDataset(df, processor, transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# =========================\n",
    "# Embedding Extraction\n",
    "# =========================\n",
    "image_embeds = []\n",
    "text_embeds = []\n",
    "label_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, text_inputs, labels in tqdm(loader, desc=\"Extracting Embeddings\"):\n",
    "        images = images.cuda()\n",
    "        input_ids = text_inputs['input_ids'].cuda()\n",
    "        attention_mask = text_inputs['attention_mask'].cuda()\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=images)\n",
    "        img_embed = F.normalize(outputs.image_embeds, p=2, dim=1)\n",
    "        txt_embed = F.normalize(outputs.text_embeds, p=2, dim=1)\n",
    "\n",
    "        image_embeds.append(img_embed.cpu())\n",
    "        text_embeds.append(txt_embed.cpu())\n",
    "        label_ids.append(labels)\n",
    "\n",
    "# =========================\n",
    "# Save Embeddings\n",
    "# =========================\n",
    "image_embeds = torch.cat(image_embeds).numpy()\n",
    "text_embeds = torch.cat(text_embeds).numpy()\n",
    "labels = torch.cat(label_ids).numpy()\n",
    "\n",
    "np.save(\"image_embeddings.npy\", image_embeds)\n",
    "np.save(\"text_embeddings.npy\", text_embeds)\n",
    "np.save(\"labels.npy\", labels)\n",
    "\n",
    "print(\"‚úÖ Embeddings saved\")\n",
    "\n",
    "# =========================\n",
    "# Evaluation Functions\n",
    "# =========================\n",
    "def recall_at_k(similarity_matrix, labels, k):\n",
    "    correct = 0\n",
    "    sorted_indices = np.argsort(similarity_matrix, axis=1)[:, ::-1]\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] in labels[sorted_indices[i, :k]]:\n",
    "            correct += 1\n",
    "    return correct / len(labels)\n",
    "\n",
    "def compute_mAP(similarity_matrix, labels):\n",
    "    aps = []\n",
    "    sorted_indices = np.argsort(similarity_matrix, axis=1)[:, ::-1]\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        y_true = (labels == labels[i]).astype(int)\n",
    "        y_score = similarity_matrix[i]\n",
    "        aps.append(average_precision_score(y_true, y_score))\n",
    "\n",
    "    return np.mean(aps)\n",
    "\n",
    "# =========================\n",
    "# Evaluation\n",
    "# =========================\n",
    "image_embeds = np.load(\"image_embeddings.npy\")\n",
    "text_embeds = np.load(\"text_embeddings.npy\")\n",
    "labels = np.load(\"labels.npy\")\n",
    "\n",
    "# Ensure unique image-text pair handling\n",
    "unique_images = {}\n",
    "for idx, label in enumerate(labels):\n",
    "    if label not in unique_images:\n",
    "        unique_images[label] = image_embeds[idx]\n",
    "image_embeds = np.array([unique_images[label] for label in labels])  # One image per label\n",
    "\n",
    "similarity = np.dot(text_embeds, image_embeds.T)\n",
    "\n",
    "print(f\"Recall@1 : {recall_at_k(similarity, labels, 1):.4f}\")\n",
    "print(f\"Recall@5 : {recall_at_k(similarity, labels, 5):.4f}\")\n",
    "print(f\"Recall@10: {recall_at_k(similarity, labels, 10):.4f}\")\n",
    "print(f\"mAP      : {compute_mAP(similarity, labels):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\cv class project\\cv_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Epoch 1/1 - Training:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuration\n",
    "class CFG:\n",
    "    model_name = \"openai/clip-vit-base-patch32\"\n",
    "    batch_size = 4\n",
    "    num_workers = 4\n",
    "    max_length = 77\n",
    "    image_size = 224\n",
    "    epochs = 1\n",
    "    lr = 5e-6\n",
    "    weight_decay = 0.01\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    data_path = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\invalid_paths.csv\"\n",
    "    checkpoint_path = \"clip_finetuned_best.pt\"\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# Custom Dataset\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.dataframe.iloc[idx]['image']  # Use full path from CSV\n",
    "        description = self.dataframe.iloc[idx]['caption']\n",
    "        label_id = self.dataframe.iloc[idx]['id']\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(image_path):\n",
    "                logging.warning(f\"Image not found: {image_path}\")\n",
    "                return None, None, None\n",
    "\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            text_inputs = self.processor(\n",
    "                text=description,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=cfg.max_length\n",
    "            )\n",
    "\n",
    "            return image, text_inputs, label_id\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading sample {idx}: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        images, text_inputs, label_ids = [], [], []\n",
    "        for image, text, label in batch:\n",
    "            if image is not None:\n",
    "                images.append(image)\n",
    "                text_inputs.append(text)\n",
    "                label_ids.append(label)\n",
    "\n",
    "        if not images:\n",
    "            return None, None, None\n",
    "\n",
    "        images = torch.stack(images)\n",
    "        input_ids = torch.cat([t['input_ids'] for t in text_inputs], dim=0)\n",
    "        attention_mask = torch.cat([t['attention_mask'] for t in text_inputs], dim=0)\n",
    "        label_ids = torch.tensor(label_ids)\n",
    "\n",
    "        return images, {'input_ids': input_ids, 'attention_mask': attention_mask}, label_ids\n",
    "\n",
    "# Data Augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((cfg.image_size, cfg.image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((cfg.image_size, cfg.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# Load and Split Data\n",
    "df = pd.read_csv(cfg.data_path)\n",
    "train_size = int(0.8 * len(df))\n",
    "train_df = df[:train_size]\n",
    "valid_df = df[train_size:]\n",
    "\n",
    "# Initialize Processor and Model\n",
    "processor = CLIPProcessor.from_pretrained(cfg.model_name)\n",
    "model = CLIPModel.from_pretrained(cfg.model_name).to(cfg.device)\n",
    "\n",
    "# Datasets and Dataloaders\n",
    "train_dataset = ImageTextDataset(train_df, processor, train_transform)\n",
    "valid_dataset = ImageTextDataset(valid_df, processor, valid_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.num_workers,\n",
    "    collate_fn=train_dataset.collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers,\n",
    "    collate_fn=valid_dataset.collate_fn\n",
    ")\n",
    "\n",
    "# Contrastive Loss\n",
    "def contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    logits = torch.matmul(image_embeds, text_embeds.T) / temperature\n",
    "    labels = torch.arange(logits.size(0)).to(cfg.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "# Training Loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
    "best_map = 0\n",
    "\n",
    "for epoch in range(cfg.epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.epochs} - Training\"):\n",
    "        if batch[0] is None:\n",
    "            continue\n",
    "\n",
    "        images, text_inputs, _ = batch\n",
    "        images = images.to(cfg.device)\n",
    "        input_ids = text_inputs['input_ids'].to(cfg.device)\n",
    "        attention_mask = text_inputs['attention_mask'].to(cfg.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=images)\n",
    "        loss = contrastive_loss(outputs.image_embeds, outputs.text_embeds)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    logging.info(f\"Epoch {epoch+1}/{cfg.epochs} - Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    image_embeds, text_embeds, label_ids = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{cfg.epochs} - Validation\"):\n",
    "            if batch[0] is None:\n",
    "                continue\n",
    "\n",
    "            images, text_inputs, ids = batch\n",
    "            images = images.to(cfg.device)\n",
    "            input_ids = text_inputs['input_ids'].to(cfg.device)\n",
    "            attention_mask = text_inputs['attention_mask'].to(cfg.device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=images)\n",
    "            image_embeds.append(outputs.image_embeds.cpu())\n",
    "            text_embeds.append(outputs.text_embeds.cpu())\n",
    "            label_ids.extend(ids.numpy())\n",
    "\n",
    "    image_embeds = torch.cat(image_embeds).numpy()\n",
    "    text_embeds = torch.cat(text_embeds).numpy()\n",
    "    label_ids = np.array(label_ids)\n",
    "\n",
    "    # Compute Metrics\n",
    "    similarity_matrix = np.dot(image_embeds, text_embeds.T)\n",
    "\n",
    "    def recall_at_k(similarity_matrix, labels, k):\n",
    "        correct = 0\n",
    "        sorted_indices = np.argsort(similarity_matrix, axis=1)[:, ::-1]\n",
    "        for i, label in enumerate(labels):\n",
    "            top_k_labels = labels[sorted_indices[i, :k]]\n",
    "            if label in top_k_labels:\n",
    "                correct += 1\n",
    "        return correct / len(labels)\n",
    "\n",
    "    def compute_mAP(similarity_matrix, labels):\n",
    "        aps = []\n",
    "        for i, label in enumerate(labels):\n",
    "            y_true = (labels == label).astype(int)\n",
    "            y_score = similarity_matrix[i]\n",
    "            aps.append(average_precision_score(y_true, y_score))\n",
    "        return np.mean(aps)\n",
    "\n",
    "    recall_1 = recall_at_k(similarity_matrix, label_ids, 1)\n",
    "    recall_5 = recall_at_k(similarity_matrix, label_ids, 5)\n",
    "    recall_10 = recall_at_k(similarity_matrix, label_ids, 10)\n",
    "    mAP = compute_mAP(similarity_matrix, label_ids)\n",
    "\n",
    "    logging.info(f\"Epoch {epoch+1}/{cfg.epochs} - Recall@1: {recall_1:.4f}, Recall@5: {recall_5:.4f}, Recall@10: {recall_10:.4f}, mAP: {mAP:.4f}\")\n",
    "\n",
    "    # Save Best Model\n",
    "    if mAP > best_map:\n",
    "        best_map = mAP\n",
    "        torch.save(model.state_dict(), cfg.checkpoint_path)\n",
    "        logging.info(f\"Saved best model with mAP: {best_map:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# Final Evaluation\n",
    "model.load_state_dict(torch.load(cfg.checkpoint_path))\n",
    "model.eval()\n",
    "image_embeds, text_embeds, label_ids = [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(valid_loader, desc=\"Final Evaluation\"):\n",
    "        if batch[0] is None:\n",
    "            continue\n",
    "\n",
    "        images, text_inputs, ids = batch\n",
    "        images = images.to(cfg.device)\n",
    "        input_ids = text_inputs['input_ids'].to(cfg.device)\n",
    "        attention_mask = text_inputs['attention_mask'].to(cfg.device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=images)\n",
    "        image_embeds.append(outputs.image_embeds.cpu())\n",
    "        text_embeds.append(outputs.text_embeds.cpu())\n",
    "        label_ids.extend(ids.numpy())\n",
    "\n",
    "image_embeds = torch.cat(image_embeds).numpy()\n",
    "text_embeds = torch.cat(text_embeds).numpy()\n",
    "label_ids = np.array(label_ids)\n",
    "\n",
    "# Save Embeddings\n",
    "np.save(\"image_embeddings_finetuned.npy\", image_embeds)\n",
    "np.save(\"text_embeddings_finetuned.npy\", text_embeds)\n",
    "np.save(\"labels_finetuned.npy\", label_ids)\n",
    "\n",
    "# Compute Final Metrics\n",
    "similarity_matrix = np.dot(image_embeds, text_embeds.T)\n",
    "recall_1 = recall_at_k(similarity_matrix, label_ids, 1)\n",
    "recall_5 = recall_at_k(similarity_matrix, label_ids, 5)\n",
    "recall_10 = recall_at_k(similarity_matrix, label_ids, 10)\n",
    "mAP = compute_mAP(similarity_matrix, label_ids)\n",
    "\n",
    "print(f\"Final Metrics:\")\n",
    "print(f\"Recall@1: {recall_1:.4f}\")\n",
    "print(f\"Recall@5: {recall_5:.4f}\")\n",
    "print(f\"Recall@10: {recall_10:.4f}\")\n",
    "print(f\"mAP: {mAP:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 863/863 [05:47<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 2.9102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 863/863 [02:35<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1 : 0.0028\n",
      "Recall@5 : 0.0115\n",
      "Recall@10: 0.0209\n",
      "mAP      : 0.0116\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# =========================\n",
    "# Load CSV (Update your path!)\n",
    "# =========================\n",
    "csv_path = r\"F:\\cv class project\\ICFG-PDES\\ICFG-PEDES\\captions_cleaned.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# =========================\n",
    "# CLIP Model Setup\n",
    "# =========================\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# =========================\n",
    "# Image Preprocessing\n",
    "# =========================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# Dataset Class\n",
    "# =========================\n",
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = row['image_path']\n",
    "        caption = row['description']\n",
    "        label_id = int(row['id'])\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        prompt = f\"A photo of a person. {caption}\"\n",
    "        text_input = self.processor(\n",
    "            text=prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77\n",
    "        )\n",
    "\n",
    "        return image, text_input, label_id\n",
    "\n",
    "# =========================\n",
    "# Collate Function\n",
    "# =========================\n",
    "def collate_fn(batch):\n",
    "    images, texts, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    input_ids = torch.cat([t['input_ids'] for t in texts], dim=0)\n",
    "    attention_mask = torch.cat([t['attention_mask'] for t in texts], dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return images, {'input_ids': input_ids, 'attention_mask': attention_mask}, labels\n",
    "\n",
    "# =========================\n",
    "# DataLoader\n",
    "# =========================\n",
    "dataset = CLIPDataset(df, processor, transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# =========================\n",
    "# Model with Projection Head\n",
    "# =========================\n",
    "class CLIPRetrievalModel(nn.Module):\n",
    "    def __init__(self, clip_model_name=\"openai/clip-vit-base-patch32\", embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(clip_model_name)\n",
    "        self.proj_image = nn.Linear(self.clip.config.projection_dim, embed_dim)\n",
    "        self.proj_text = nn.Linear(self.clip.config.projection_dim, embed_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        outputs = self.clip(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        img_embed = F.normalize(self.proj_image(outputs.image_embeds), p=2, dim=1)\n",
    "        txt_embed = F.normalize(self.proj_text(outputs.text_embeds), p=2, dim=1)\n",
    "        return img_embed, txt_embed\n",
    "\n",
    "# =========================\n",
    "# Training Loop\n",
    "# =========================\n",
    "model = CLIPRetrievalModel().cuda()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, text_inputs, labels in tqdm(loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = text_inputs['input_ids'].cuda()\n",
    "        attention_mask = text_inputs['attention_mask'].cuda()\n",
    "        images = images.cuda()\n",
    "\n",
    "        img_embed, txt_embed = model(input_ids, attention_mask, images)\n",
    "\n",
    "        logits_per_image = img_embed @ txt_embed.T\n",
    "        logits_per_text = txt_embed @ img_embed.T\n",
    "        ground_truth = torch.arange(len(images)).cuda()\n",
    "\n",
    "        loss_img = loss_fn(logits_per_image, ground_truth)\n",
    "        loss_txt = loss_fn(logits_per_text, ground_truth)\n",
    "        loss = (loss_img + loss_txt) / 2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(loader):.4f}\")\n",
    "\n",
    "# =========================\n",
    "# Save Fine-Tuned Model\n",
    "# =========================\n",
    "torch.save(model.state_dict(), \"clip_finetuned.pt\")\n",
    "\n",
    "# =========================\n",
    "# Evaluation Functions\n",
    "# =========================\n",
    "def recall_at_k(similarity_matrix, labels, k):\n",
    "    correct = 0\n",
    "    sorted_indices = np.argsort(similarity_matrix, axis=1)[:, ::-1]\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] in labels[sorted_indices[i, :k]]:\n",
    "            correct += 1\n",
    "    return correct / len(labels)\n",
    "\n",
    "def compute_mAP(similarity_matrix, labels):\n",
    "    aps = []\n",
    "    sorted_indices = np.argsort(similarity_matrix, axis=1)[:, ::-1]\n",
    "    for i in range(len(labels)):\n",
    "        y_true = (labels == labels[i]).astype(int)\n",
    "        y_score = similarity_matrix[i]\n",
    "        aps.append(average_precision_score(y_true, y_score))\n",
    "    return np.mean(aps)\n",
    "\n",
    "# =========================\n",
    "# Inference and Evaluation\n",
    "# =========================\n",
    "model.eval()\n",
    "image_embeds = []\n",
    "text_embeds = []\n",
    "label_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, text_inputs, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "        input_ids = text_inputs['input_ids'].cuda()\n",
    "        attention_mask = text_inputs['attention_mask'].cuda()\n",
    "        images = images.cuda()\n",
    "\n",
    "        img_embed, txt_embed = model(input_ids, attention_mask, images)\n",
    "\n",
    "        image_embeds.append(img_embed.cpu())\n",
    "        text_embeds.append(txt_embed.cpu())\n",
    "        label_ids.append(labels)\n",
    "\n",
    "image_embeds = torch.cat(image_embeds).numpy()\n",
    "text_embeds = torch.cat(text_embeds).numpy()\n",
    "labels = torch.cat(label_ids).numpy()\n",
    "\n",
    "# Handle multiple captions per image (optional simplification)\n",
    "unique_images = {}\n",
    "for idx, label in enumerate(labels):\n",
    "    if label not in unique_images:\n",
    "        unique_images[label] = image_embeds[idx]\n",
    "image_embeds = np.array([unique_images[label] for label in labels])\n",
    "\n",
    "similarity = np.dot(text_embeds, image_embeds.T)\n",
    "\n",
    "print(f\"Recall@1 : {recall_at_k(similarity, labels, 1):.4f}\")\n",
    "print(f\"Recall@5 : {recall_at_k(similarity, labels, 5):.4f}\")\n",
    "print(f\"Recall@10: {recall_at_k(similarity, labels, 10):.4f}\")\n",
    "print(f\"mAP      : {compute_mAP(similarity, labels):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 863/863 [06:12<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 2.9317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 863/863 [02:36<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1 : 0.0022\n",
      "Best Recall@1: 0.0022\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Training Loop\n",
    "# =========================\n",
    "best_recall1 = 0  # Variable to track the best Recall@1 score\n",
    "model = CLIPRetrievalModel().cuda()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, text_inputs, labels in tqdm(loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = text_inputs['input_ids'].cuda()\n",
    "        attention_mask = text_inputs['attention_mask'].cuda()\n",
    "        images = images.cuda()\n",
    "\n",
    "        img_embed, txt_embed = model(input_ids, attention_mask, images)\n",
    "\n",
    "        logits_per_image = img_embed @ txt_embed.T\n",
    "        logits_per_text = txt_embed @ img_embed.T\n",
    "        ground_truth = torch.arange(len(images)).cuda()\n",
    "\n",
    "        loss_img = loss_fn(logits_per_image, ground_truth)\n",
    "        loss_txt = loss_fn(logits_per_text, ground_truth)\n",
    "        loss = (loss_img + loss_txt) / 2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(loader):.4f}\")\n",
    "\n",
    "    # =========================\n",
    "    # Inference and Evaluation\n",
    "    # =========================\n",
    "    model.eval()\n",
    "    image_embeds = []\n",
    "    text_embeds = []\n",
    "    label_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, text_inputs, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = text_inputs['input_ids'].cuda()\n",
    "            attention_mask = text_inputs['attention_mask'].cuda()\n",
    "            images = images.cuda()\n",
    "\n",
    "            img_embed, txt_embed = model(input_ids, attention_mask, images)\n",
    "\n",
    "            image_embeds.append(img_embed.cpu())\n",
    "            text_embeds.append(txt_embed.cpu())\n",
    "            label_ids.append(labels)\n",
    "\n",
    "    image_embeds = torch.cat(image_embeds).numpy()\n",
    "    text_embeds = torch.cat(text_embeds).numpy()\n",
    "    labels = torch.cat(label_ids).numpy()\n",
    "\n",
    "    # Handle multiple captions per image (optional simplification)\n",
    "    unique_images = {}\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label not in unique_images:\n",
    "            unique_images[label] = image_embeds[idx]\n",
    "    image_embeds = np.array([unique_images[label] for label in labels])\n",
    "\n",
    "    similarity = np.dot(text_embeds, image_embeds.T)\n",
    "\n",
    "    recall1 = recall_at_k(similarity, labels, 1)\n",
    "    print(f\"Recall@1 : {recall1:.4f}\")\n",
    "\n",
    "    # Save the best model based on Recall@1\n",
    "    if recall1 > best_recall1:\n",
    "        best_recall1 = recall1\n",
    "        torch.save(model.state_dict(), \"model.pt\")  # Save the best model\n",
    "\n",
    "print(f\"Best Recall@1: {best_recall1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 31296,
     "sourceId": 39911,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
